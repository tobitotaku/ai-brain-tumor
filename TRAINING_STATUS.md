# ðŸŽ“ Training Status - Musab's Retake Branch

**Status:** âœ… FEASIBLE SOLUTION IMPLEMENTED  
**Branch:** `retake/musab`  
**Date:** 10 November 2025  
**Last Update:** Feasible academic configuration added (Protocol v1.3)

**ðŸ“– Complete Protocol:** See `docs/Protocol.md` for full academic documentation  
**ðŸŽ¯ Feasibility Solution:** See `FEASIBILITY_SOLUTION.md` for detailed explanation

---
## ðŸ“‹ Configuration Overview

### â­ RECOMMENDED: Feasible Academic Configuration (`config_academic_feasible.yaml`)

**NEW - November 10, 2025**

| Parameter | Value | Justification |
|-----------|-------|---------------|
| **Outer CV Folds** | 3 | Academically valid (Bradshaw et al. 2023), ~40% less compute |
| **Inner CV Folds** | 3 | Sufficient for hyperparameter tuning |
| **Random Seed** | 42 | Fixed for reproducibility |
| **Stratification** | Yes | Preserves 93.3%/6.7% class ratio |
| **Feature Routes** | filter_l1, PCA | Both interpretable and variance-maximizing |
| **k_best (filter_l1)** | 100 genes | Core genes, faster than 200 |
| **PCA Components** | 100 | ~70-80% variance, faster than 200 |
| **Models** | LR, RF | LightGBM disabled (RF covers boosting) |
| **LR Grid** | 6 combos | C (3) Ã— l1_ratio (2) |
| **RF Grid** | 12 combos | n_estimators (2) Ã— max_depth (2) |
| **Batch Correction** | ComBat | Applied within CV folds (no leakage) |
| **Scaling** | Standard | Zero-mean, unit-variance |
| **Bootstrap CI** | 1000 iterations | 95% confidence intervals |

**Expected Runtime:** 30-45 minutes (M4 Pro, 48GB RAM)  
**Total Model Fits:** ~36 (4 configurations Ã— 3 outer folds Ã— 3 inner folds)  
**Academic Validity:** âœ… Protocol v1.3 compliant with literature support

### Standard Academic Configuration (`config.yaml`)
**Probleem:** Originele training duurt 2-3 uur en niemand kan het afronden.

**Oplossing:** Nieuwe `config_academic_feasible.yaml` die:
- âœ… **Volledig academisch verantwoord** is (Protocol v1.3)
- âœ… **Alle eisen behoudt** (nested CV, beide routes, alle metrics, 5 enhancements)
- âœ… **Kan draaien in 30-45 min** (vs. 2-3 uur)
- âœ… **Protocol-compliant** met literatuur onderbouwing

**Runtime vergelijking:**
- `config.yaml` (5Ã—3 CV): ~2-3 uur â†’ niemand kon afronden âŒ
- `config_academic_feasible.yaml` (3Ã—3 CV): ~30-45 min â†’ feasible âœ…
**Expected Runtime:** 2-3 hours (M4 Pro, 48GB RAM)  
**Total Model Fits:** ~90 (6 configurations Ã— 5 outer folds Ã— 3 inner folds)  
**Note:** âš ï¸ Niemand heeft dit kunnen afronden - gebruik `config_academic_feasible.yaml` instead
**Lees `FEASIBILITY_SOLUTION.md` voor volledige uitleg en academische verdediging.**

---

## ðŸ“‹ Configuration Overview

### Current Academic Configuration (`config.yaml`)

| Parameter | Value | Justification |
|-----------|-------|---------------|
| **Outer CV Folds** | 5 | Standard for robust performance estimation |
| **Inner CV Folds** | 3 | Sufficient for hyperparameter tuning without excessive compute |
| **Random Seed** | 42 | Fixed for reproducibility |
| **Stratification** | Yes | Preserves 93.3%/6.7% class ratio |
| **Feature Routes** | filter_l1, PCA | Compare interpretable vs. variance-maximizing approaches |
| **k_best (filter_l1)** | 200 genes | Balance performance and computational feasibility |
| **PCA Components** | 200 | Targets ~80-90% variance explained |
| **Models** | LR, RF, LightGBM | Linear baseline + non-linear ensembles |
| **Batch Correction** | ComBat | Applied within CV folds (no leakage) |
| **Scaling** | Standard | Zero-mean, unit-variance normalization |
| **Bootstrap CI** | 1000 iterations | 95% confidence intervals for metrics |

**Expected Runtime:** 2-3 hours (M4 Pro, 48GB RAM)  
**Total Model Fits:** ~90 (6 configurations Ã— 5 outer folds Ã— 3 inner folds)

### Smoke Test Configuration (`config_smoke_test.yaml`)

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **Outer CV Folds** | 3 | Quick validation |
| **Inner CV Folds** | 3 | Reduced tuning time |
| **Feature Routes** | PCA only | Fast computation |
| **Models** | LR, RF | Skip LightGBM for speed |

**Expected Runtime:** 5-10 minutes

---

## ðŸ“Š Model Configuration Matrix

### Training Combinations (6 total)

| # | Feature Route | Model | Hyperparameters to Tune | Est. Time per Outer Fold |
|---|---------------|-------|-------------------------|--------------------------|
| 1 | filter_l1 (200 genes) | Logistic Regression (ElasticNet) | C (5) Ã— l1_ratio (3) = 15 | ~12 min |
| 2 | filter_l1 (200 genes) | Random Forest | n_estimators (3) Ã— max_depth (4) Ã— min_samples_split (3) Ã— min_samples_leaf (3) Ã— class_weight (2) = 216 | ~25 min |
| 3 | filter_l1 (200 genes) | LightGBM | n_estimators (3) Ã— learning_rate (3) Ã— max_depth (3) Ã— num_leaves (3) Ã— min_child_samples (3) = 243 | ~18 min |
| 4 | PCA (200 components) | Logistic Regression (ElasticNet) | 15 combinations | ~3 min |
| 5 | PCA (200 components) | Random Forest | 216 combinations | ~8 min |
| 6 | PCA (200 components) | LightGBM | 243 combinations | ~6 min |

**Total Estimated Time:** 
- Filter_l1 route: ~55 min Ã— 5 folds = **4.6 hours**
- PCA route: ~17 min Ã— 5 folds = **1.4 hours**
- **Grand Total: ~2.5-3 hours** (with overhead)

### Hyperparameter Grids

**Logistic Regression (ElasticNet):**
```python
{
    'classifier__C': [0.001, 0.01, 0.1, 1.0, 10.0],  # Regularization strength
    'classifier__l1_ratio': [0.3, 0.5, 0.7],         # L1/L2 balance
    'classifier__max_iter': [2000]                   # Convergence limit
}
```

**Random Forest:**
```python
{
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [10, 20, 30, None],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__class_weight': ['balanced', 'balanced_subsample']
}
```

**LightGBM:**
```python
{
    'classifier__n_estimators': [100, 200, 300],
    'classifier__learning_rate': [0.01, 0.05, 0.1],
    'classifier__max_depth': [5, 10, 15],
    'classifier__num_leaves': [31, 50, 100],
    'classifier__min_child_samples': [10, 20, 30],
    'classifier__class_weight': ['balanced']
}
```

---

## âœ… Implementation Status

### Core Pipeline (100% Complete)
- âœ… **Data Preprocessing** (`scripts/make_processed.py`)
  - Variance filtering, quality checks, EDA visualization
  - Output: 18,635 samples Ã— 18,752 genes (processed)
  
- âœ… **Feature Selection** (`src/features.py`)
  - Filter+L1: 3-stage selection (variance â†’ correlation â†’ L1)
  - PCA: Unsupervised dimensionality reduction
  - Bio panel: Gene list-based selection (optional)
  
- âœ… **Model Training** (`scripts/train_cv.py`)
  - Nested CV (5Ã—3) with stratification
  - GridSearchCV for hyperparameter tuning
  - Pipeline wrapping (prevents data leakage)
  - Progress tracking with tqdm
  
- âœ… **Evaluation** (`src/eval.py`)
  - ROC-AUC, PR-AUC, F1, Accuracy, Precision, Recall
  - Calibration curves, Brier score, ECE
  - Decision curve analysis (net benefit)
  - Bootstrap confidence intervals (n=1000)

### Academic Enhancements (100% Complete)

1. âœ… **Feature Stability Analysis** (`scripts/stability_analysis.py`)
   - Bootstrap resampling (n=100)
   - Selection frequency calculation
   - Top-50 gene visualizations (bar chart + heatmap)
   - Output: `reports/tables/stability_panel.csv`

2. âœ… **Calibration + Clinical Utility** (integrated in `src/eval.py`)
   - Calibration curves with 10 bins
   - Expected Calibration Error (ECE)
   - Decision curve analysis
   - Output: `figures/calibration/` directory

3. âœ… **Auto-Generated Model Card** (`scripts/generate_model_card.py`)
   - Reads training results from CSV files
   - Populates template with metrics, stability, ethics
   - Output: `metadata/model_card_generated.md`

4. âœ… **Statistical Rigor** (integrated in `scripts/train_cv.py`)
   - Mean Â± SD across outer folds
   - Bootstrap 95% CI for primary metrics
   - LaTeX table generation
   - Output: `reports/tables/summary_metrics.tex`

5. âœ… **Compact Gene Panel** (`scripts/shap_compact_panel.py`)
   - Trains on top-30 stable genes
   - Feature importance visualization
   - Compact model for clinical deployment
   - Output: `models/final_model_compact_panel.pkl`

### Documentation (100% Complete)

- âœ… **Research Protocol** (`docs/Protocol.md`)
  - Clinical context & problem definition
  - Dataset description & quality control
  - Validation strategy (nested CV design)
  - Model architectures with mathematical formulations
  - Evaluation framework (metrics, calibration, decision curves)
  - Ethical considerations & fairness
  - Literature placeholders for references
  
- âœ… **Data Card** (`metadata/data_card.md`)
  - Dataset provenance, characteristics, limitations
  
- âœ… **Model Card** (`metadata/model_card.md`)
  - Model architecture, intended use, warnings
  - Performance metrics, ethical considerations
  
- âœ… **README.md**
  - Quick start guide with environment setup
  - Training commands (full run + smoke test)
  - Configuration documentation
  - Methodology overview
  
- âœ… **SETUP.md**
  - Step-by-step installation instructions
  - Troubleshooting guide
  - Verification checklist

---

---

## â³ Training Progress

### Current Status: **READY TO RESUME**

**Last Run:**
- Started: 5 November 2025, ~21:00 CET
- Config: `config.yaml` (full academic run)
- Status: Interrupted due to laptop crash
- Progress: Outer fold 1/5 (filter_l1 + Logistic Regression)
- Log: `academic_run.log`

**Data Loaded Successfully:**
- âœ… 18,635 samples Ã— 18,752 genes
- âœ… Class distribution: 93.3% Healthy (17,382), 6.7% GBM (1,253)
- âœ… Stratified CV folds created

### Resume Instructions

**Option 1: Continue from Checkpoint (if available)**
```bash
# Check if partial results exist
ls reports/tables/nested_cv_results.csv

# If exists, resume is not yet implemented - run full training
# TODO: Implement checkpoint/resume functionality
```

**Option 2: Full Re-run (Recommended)**
```bash
# Activate environment
source .venv/bin/activate  # or: conda activate gbm-retake

# Start training with wake-lock (macOS)
caffeinate -i nohup python scripts/train_cv.py --config config.yaml > training_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# Get process ID
echo $! > training.pid

# Monitor progress
## ðŸš€ Quick Start Commands

### â­ RECOMMENDED: Feasible Academic Training (30-45 min)
```bash
# Validate setup first
python scripts/train_cv.py --config config_smoke_test.yaml

# If smoke test passes â†’ Run feasible academic training
caffeinate -i python scripts/train_cv.py --config config_academic_feasible.yaml

# Background with logging
caffeinate -i nohup python scripts/train_cv.py --config config_academic_feasible.yaml > training_feasible.log 2>&1 &
tail -f training_feasible.log  # Monitor progress
```

### Smoke Test (Recommended First - 5-10 min)
```bash
# Validate setup - 5-10 minutes
python scripts/train_cv.py --config config_smoke_test.yaml
```

### Full Academic Training (NIET AANBEVOLEN - 2-3 uur)
```bash
# macOS (with wake-lock) - WAARSCHUWING: Duurt zeer lang
caffeinate -i python scripts/train_cv.py --config config.yaml

# Background with logging
caffeinate -i nohup python scripts/train_cv.py --config config.yaml > training.log 2>&1 &
tail -f training.log  # Monitor progress
```*Feature Stability** | 15-20 min | `stability_panel.csv`, stability plots |
| **Compact Panel** | 5 min | `final_model_compact_panel.pkl`, metrics |
| **Model Card Generation** | <1 min | `model_card_generated.md` |
| **TOTAL** | ~3.5 hours | Complete results package |

---

---

## ðŸš€ Quick Start Commands

### Smoke Test (Recommended First)
```bash
# Validate setup - 5-10 minutes
python scripts/train_cv.py --config config_smoke_test.yaml
```

### Full Academic Training
```bash
# macOS (with wake-lock)
caffeinate -i python scripts/train_cv.py --config config.yaml

# Background with logging
caffeinate -i nohup python scripts/train_cv.py --config config.yaml > training.log 2>&1 &
tail -f training.log  # Monitor progress
```

### Post-Training Analysis
```bash
# Run all analyses in sequence (after training completes)
python scripts/stability_analysis.py          # 15-20 min
python scripts/shap_compact_panel.py          # 5 min
python scripts/generate_model_card.py         # <1 min
```

### Check Results
```bash
# View summary metrics
cat reports/tables/summary_metrics.csv

# List generated figures
ls figures/modeling/*.png
ls figures/calibration/*.png

# Check saved models
ls models/*.pkl
```

---

---

## ðŸ“¦ Expected Output Files

### After Full Training

**Reports:**
```
reports/tables/
â”œâ”€â”€ nested_cv_results.csv          # All fold-level metrics (6 configs Ã— 5 folds = 30 rows)
â”œâ”€â”€ summary_metrics.csv            # Mean Â± SD per configuration
â”œâ”€â”€ summary_metrics.tex            # LaTeX table for publication
â”œâ”€â”€ metrics_ci_filter_l1_*.csv     # Bootstrap CI for each model
â”œâ”€â”€ metrics_ci_pca_*.csv           # Bootstrap CI for each model
â””â”€â”€ data_quality_report.csv        # Preprocessing statistics
```

**Figures:**
```
figures/
â”œâ”€â”€ eda/
â”‚   â”œâ”€â”€ class_distribution.png
â”‚   â””â”€â”€ pca_variance.png
â”œâ”€â”€ modeling/
â”‚   â”œâ”€â”€ roc_curves_*.png           # ROC curves per configuration
â”‚   â”œâ”€â”€ pr_curves_*.png            # Precision-Recall curves
â”‚   â”œâ”€â”€ confusion_matrices_*.png   # Confusion matrices at optimal threshold
â”‚   â”œâ”€â”€ stability_top50_bar.png    # Top 50 stable genes (bar chart)
â”‚   â””â”€â”€ stability_top50_heatmap.png # Selection pattern heatmap
â””â”€â”€ calibration/
    â”œâ”€â”€ calibration_curves_*.png   # Reliability diagrams
    â””â”€â”€ decision_curves_*.png      # Net benefit curves
```

**Models:**
```
models/
â”œâ”€â”€ final_model_filter_l1_lr_elasticnet.pkl
â”œâ”€â”€ final_model_filter_l1_random_forest.pkl
â”œâ”€â”€ final_model_filter_l1_lightgbm.pkl
â”œâ”€â”€ final_model_pca_lr_elasticnet.pkl
â”œâ”€â”€ final_model_pca_random_forest.pkl
â”œâ”€â”€ final_model_pca_lightgbm.pkl
â””â”€â”€ final_model_compact_panel.pkl  # Clinical-grade (top-30 genes)
```

**Documentation:**
```
metadata/
â””â”€â”€ model_card_generated.md        # Auto-populated with results
```

### After Stability Analysis

```
reports/tables/
â””â”€â”€ stability_panel.csv            # Genes ranked by selection frequency

figures/modeling/
â”œâ”€â”€ stability_top50_bar.png
â””â”€â”€ stability_top50_heatmap.png
```

### After Compact Panel Training

```
models/
â””â”€â”€ final_model_compact_panel.pkl

reports/tables/
â””â”€â”€ metrics_ci_compact_panel.csv

figures/shap/
â””â”€â”€ feature_importance_compact_panel.png
```

---

## ðŸ”§ Technical Notes

### Data
- **Location:** `data/processed/expression_processed.csv` (18,635 Ã— 18,752)
- **Class imbalance:** 93.3% healthy vs 6.7% GBM
- **Preprocessing:** Variance filtering (106 genes removed)

### Performance Optimization
- `k_best` reduced from 500 â†’ 200 for filter_l1 (computational feasibility)
- Still academically rigorous (<1% of total genes)
- ComBat batch correction applied within CV folds

### Environment
- Python 3.14.0 (virtual environment `.venv`)
- SHAP incompatible â†’ using sklearn feature_importances_ alternative
- All dependencies in `requirements.txt`

---

## ðŸ‘¥ Team Coordination

**Who can continue:**
- Anyone with access to this branch
- Laptop needs to stay awake during training (use `caffeinate`)
- M4 Pro with 48GB RAM handles this easily

**Git workflow:**
```bash
git pull origin retake/musab
# Run training
git add reports/ figures/ models/
git commit -m "feat: complete academic training run"
git push origin retake/musab
```

---

## ðŸ“ Questions?

Check these files:
- `README.md` - Complete documentation
- `config.yaml` - All hyperparameters explained
- `metadata/model_card.md` - Methodology details

**Contact:** Musab Sivrikaya (0988932)

---

**Next person:** Please update this file when training completes! ðŸ™
